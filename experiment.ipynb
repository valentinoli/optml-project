{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture model experiment\n",
    "In this notebook we perform the experiment to verify that we indeed see an exponential amount of gradient queries for stochastic gradient descent with an increasing parameter $d$, whereas we observe a linear relation in the case of the Metropolis-adjusted Langevin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from src.mixture import GaussianMixture, random_ball, DirichletMixture\n",
    "from src.optimizer import em, ula\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the data\n",
    "We begin by generating the datasets used in our evaluation. Since we are trying various distributions, we have to make sure that they are all parameterized properly so as to achieve the following properties:\n",
    "1. The clusters need to be adequately separated.\n",
    "2. The amount of clusters in our dataset needs to be few ($M=\\log_2 d$ in paper).\n",
    "\n",
    "Something we could consider trying out would be to violate these properties to see what the failure mode is.\n",
    "\n",
    "In our experiment we use the following distributions (we always let $N=2^d$):\n",
    "1. Gaussian $\\sigma = 1 / \\sqrt{d}, M=\\log_2 d$\n",
    "2. Dirichlet\n",
    "3. Exponential\n",
    "4. Student's T (included since not log-concave for all parameters)\n",
    "\n",
    "For our experiment to work, we need to generate new such problems for an increasing parameter $d$. Therefore we will need in total $4d$ datasets to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "Now we begin the experiment. The setup is quite simplistic:\n",
    "1. Iterate over parameter $d$.\n",
    "2. Gather our distribution datasets for that parameter $d$.\n",
    "3. Estimate the parameters of the distribution using expectation-maximization.\n",
    "4. Estimate the parameters of the distribution using MALA.\n",
    "5. Save the amount of gradient queries required for both approaches.\n",
    "6. Create line plot of required gradient queries for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points_in_ball(num_points: int = 1000, radius: int = 1, dim: int = 2):\n",
    "    if dim < 2 or dim > 3:\n",
    "        raise ValueError('Invalid dimension')\n",
    "\n",
    "    points = random_ball(num_points, dim, radius=radius)\n",
    "\n",
    "    subplot_kw = {}\n",
    "\n",
    "    if dim == 3:\n",
    "        subplot_kw=dict(projection='3d')\n",
    "\n",
    "    fig, ax = plt.subplots(subplot_kw=subplot_kw)\n",
    "\n",
    "    if dim == 2:\n",
    "        ax.set_aspect('equal')\n",
    "        patch = Circle((0, 0), radius, fill=False, ls='-', lw=0.25)\n",
    "        ax.add_patch(patch)\n",
    "        ax.axvline(c='grey', lw=0.5)\n",
    "        ax.axhline(c='grey', lw=0.5)\n",
    "\n",
    "    ax.scatter(*np.split(points, dim, axis=1), marker='.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points_in_ball(radius=2, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points_in_ball(radius=2, dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(ax, points, marker = 'o', c = None, s = None):\n",
    "    ax.scatter(*np.split(points, 2, axis=1), edgecolors=None, c=c, s=s, marker=marker)\n",
    "    ax.axvline(c='grey', lw=0.5)\n",
    "    ax.axhline(c='grey', lw=0.5)\n",
    "\n",
    "def plot_axis(ax, model):\n",
    "    ax.set_aspect('equal')\n",
    "    patch = Circle((0, 0), radius=model.R, fill=False, ls='-', lw=0.25)\n",
    "    ax.add_patch(patch)\n",
    "    plot_scatter(ax, model.points, c='b', s=100)\n",
    "    plot_scatter(ax, model.params, c='r', marker='o', s=30)\n",
    "                \n",
    "def plot_gmm_initializations_2d_2axes():\n",
    "    gmm_init_from_data = GaussianMixture(2)\n",
    "    gmm_init_random_ball = GaussianMixture(2, init_from_data=False)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "    \n",
    "    ax1.set_title('Initialization from data points')\n",
    "    ax2.set_title('Random initialization within ball')\n",
    "    \n",
    "    plot_axis(ax1, gmm_init_from_data)\n",
    "    plot_axis(ax2, gmm_init_random_ball)\n",
    "    \n",
    "    # Custom legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', label='Points', markerfacecolor='b', markersize=10),\n",
    "        Line2D([0], [0], marker='o', color='w', label='Mean parameter', markerfacecolor='r', markersize=10)\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm_initializations_2d_2axes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(ax, points, marker = 'o', c = None, s = None):\n",
    "    ax.scatter(*np.split(points, 2, axis=1), c=c, s=s, marker=marker)\n",
    "\n",
    "def plot_gmm_initializations_2d():\n",
    "    gmm = GaussianMixture(2)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # plot x and y axes\n",
    "    ax.axvline(c='grey', lw=0.5)\n",
    "    ax.axhline(c='grey', lw=0.5)\n",
    "    \n",
    "    # plot ball of radius R\n",
    "    patch = Circle((0, 0), radius=gmm.R, fill=False, ls='-', lw=0.25)\n",
    "    ax.add_patch(patch)\n",
    "    \n",
    "    # plot points\n",
    "    plot_scatter(ax, gmm.points, c='b', s=100)\n",
    "    \n",
    "    # plot two types of mean initializations\n",
    "    plot_scatter(ax, gmm.init_params(from_data=False), c='g', s=30)\n",
    "    plot_scatter(ax, gmm.init_params(from_data=True), c='r', s=30)\n",
    "    \n",
    "    ax.set_title('Mean parameter initialization schemes for GMM (d=2)')\n",
    "    \n",
    "    # Custom legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', label='Points', markerfacecolor='b', markersize=10),\n",
    "        Line2D([0], [0], marker='o', color='w', label='Random initialization', markerfacecolor='g', markersize=10),\n",
    "        Line2D([0], [0], marker='o', color='w', label='Initialization from data', markerfacecolor='r', markersize=10)\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm_initializations_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 5000\n",
    "MAX_ESTIMATION_ITERATIONS = 1000000\n",
    "EM_ESTIMATE_THRESHOLD = 1e-8\n",
    "EM_ERROR_THRESHOLD = 1e-6\n",
    "ULA_PARAM_ESTIMATE_THRESHOLD = 1e-5\n",
    "ULA_OBJECTIVE_ESTIMATE_THRESHOLD = 1e-8\n",
    "FROM_DATA = True\n",
    "NB_TRIALS = 10\n",
    "NB_TRIALS_ESTIMATE = 20\n",
    "NB_EXPS_ULA = 1\n",
    "MAX_D = 8\n",
    "ERROR_GRADIENT_ESTIMATE = 1e-4\n",
    "GAMMA = None\n",
    "\n",
    "dims = range(2, MAX_D+1)\n",
    "\n",
    "models = {\n",
    "    m.__name__: {\n",
    "        'constructor': m,\n",
    "        'df': None\n",
    "    }\n",
    "    for m in [\n",
    "        GaussianMixture\n",
    "    ]\n",
    "}\n",
    "\n",
    "for name, model_dict in models.items():\n",
    "    # Record results\n",
    "    model_results = []\n",
    "\n",
    "    \n",
    "    # Default iterations to start with\n",
    "    iterations_em = 2\n",
    "    iterations_ula = 2\n",
    "    \n",
    "    # Run for rest of dimensions\n",
    "    for d in tqdm(dims):\n",
    "        # Create finite mixture model problem\n",
    "        model = model_dict['constructor'](d)\n",
    "        \n",
    "        # Run EM\n",
    "        if iterations_em < MAX_ITERATIONS:\n",
    "            \n",
    "            # Estimate underlying set of parameters\n",
    "            em_true_params = None\n",
    "            em_true_params_found = False\n",
    "            em_true_params_iterations = iterations_em*1000\n",
    "            while not em_true_params_found:\n",
    "                \n",
    "                # Assume we have enough iterations until proven otherwise\n",
    "                em_true_params_found = True\n",
    "                \n",
    "                # Run through trials\n",
    "                em_estimate_params = np.zeros((NB_TRIALS_ESTIMATE, em_true_params_iterations, \n",
    "                                               model.params.shape[0], model.params.shape[1]))\n",
    "                em_estimate_objective = np.zeros((NB_TRIALS_ESTIMATE, em_true_params_iterations))\n",
    "                for i in range(NB_TRIALS_ESTIMATE):\n",
    "                    \n",
    "                    # Exit if we failed for given amount of iterations already\n",
    "                    if not em_true_params_found:\n",
    "                        break\n",
    "                    \n",
    "                    # Reset parameters\n",
    "                    model.reset()\n",
    "                    \n",
    "                    # Run EM\n",
    "                    em_param_iterates, em_objective_iterates = em(model, em_true_params_iterations)\n",
    "                    em_estimate_params[i] = em_param_iterates\n",
    "                    em_estimate_objective[i] = em_objective_iterates\n",
    "                    \n",
    "                    # Make sure we don't differ too much from previous estimates\n",
    "                    for j in range(i+1):\n",
    "                        \n",
    "                        # If differ by more than threshold, we reset and increase amount of iterations\n",
    "                        delta = np.linalg.norm(em_estimate_params[j][-1] - em_estimate_params[i][-1], 1)\n",
    "                        if delta > EM_ESTIMATE_THRESHOLD:\n",
    "                            em_true_params_iterations *= 10\n",
    "                            em_true_params_found = False\n",
    "                            print(f'Delta: {delta}')\n",
    "                            print(f'Iterations now: {em_true_params_iterations}')\n",
    "                            break\n",
    "                        \n",
    "                # Just choose any of the 20 iterations\n",
    "                em_true_params = em_estimate_params[-1]\n",
    "                em_true_params_objective = em_estimate_objective[-1]\n",
    "                \n",
    "            # Repeat for the specified amount of trials\n",
    "            for _ in range(NB_TRIALS):\n",
    "            \n",
    "                # Record how many iterations were required\n",
    "                model.reset()\n",
    "                iterations_em = len(em(model, MAX_ITERATIONS, em_true_params_objective[-1], EM_ERROR_THRESHOLD)[0])\n",
    "                model_results.append({'Dimensions': d, 'Algorithm': 'EM', 'Gradient queries': iterations_em})\n",
    "                \n",
    "        # Run ULA (disabled as we couldn't get it to converge in the end)\n",
    "        if False and iterations_ula < MAX_ITERATIONS:\n",
    "            \n",
    "            # Estimate underlying set of parameters\n",
    "            ula_expected_params = None\n",
    "            ula_expected_objective = None\n",
    "            ula_expected_params_found = False\n",
    "            ula_expected_params_iterations = iterations_ula*1000\n",
    "            if ula_expected_params_iterations > MAX_ESTIMATION_ITERATIONS:\n",
    "                raise TimeoutError\n",
    "            while not ula_expected_params_found:\n",
    "                \n",
    "                # Assume we have enough iterations until proven otherwise\n",
    "                ula_expected_params_found = True\n",
    "                \n",
    "                # Run through trials\n",
    "                ula_estimate_params = np.zeros((NB_TRIALS_ESTIMATE,\n",
    "                                               model.params.shape[0], model.params.shape[1]))\n",
    "                ula_estimate_objective = np.zeros(NB_TRIALS_ESTIMATE)\n",
    "                for i in range(NB_TRIALS_ESTIMATE):\n",
    "                    \n",
    "                    # Reset parameters\n",
    "                    model.reset(FROM_DATA)\n",
    "                    \n",
    "                    # Run ULA\n",
    "                    ula_param_iterates, ula_objective_iterates = ula(model, \n",
    "                                                                     ula_expected_params_iterations,\n",
    "                                                                     NB_EXPS_ULA,\n",
    "                                                                     ERROR_GRADIENT_ESTIMATE,\n",
    "                                                                     gamma=GAMMA)\n",
    "                    \n",
    "                    # Compute expectation on parameters\n",
    "                    ula_estimate_params[i] = np.mean(ula_param_iterates)\n",
    "                    ula_estimate_objective[i] = np.mean(ula_objective_iterates)\n",
    "                    \n",
    "                    # Make sure we don't differ too much from previous estimates\n",
    "                    for j in range(i+1):\n",
    "                        \n",
    "                        # If differ by more than threshold, we reset and increase amount of iterations\n",
    "                        delta_params = np.linalg.norm(ula_estimate_params[j] - ula_estimate_params[i], 1)\n",
    "                        delta_objective = abs(ula_estimate_objective[j] - ula_estimate_objective[i])\n",
    "                        if delta_params > ULA_PARAM_ESTIMATE_THRESHOLD or delta_objective > ULA_OBJECTIVE_ESTIMATE_THRESHOLD:\n",
    "                            ula_expected_params_iterations *= 10\n",
    "                            print(f'Delta params: {delta_params}')\n",
    "                            print(f'Delta objective: {delta_objective}')\n",
    "                            print(f'Iterations now: {ula_expected_params_iterations}')\n",
    "                            ula_expected_params_found = False\n",
    "                            break\n",
    "                        \n",
    "                # Just choose any of the 20 iterations\n",
    "                ula_expected_params = ula_estimate_params[-1]\n",
    "                ula_expected_objective = ula_estimate_objective[-1]\n",
    "                \n",
    "            # Repeat for the specified amount of trials\n",
    "            for _ in range(NB_TRIALS):\n",
    "            \n",
    "                # Record how many iterations were required\n",
    "                model.reset(FROM_DATA)\n",
    "                iterations_ula = len(ula(model, MAX_ITERATIONS, NB_EXPS_ULA, ERROR_GRADIENT_ESTIMATE, \n",
    "                                        exp_mu=ula_expected_params, exp_U=ula_expected_objective, \n",
    "                                         timeout=MAX_ITERATIONS, gamma=GAMMA)[0])\n",
    "                model_results.append({'Dimensions': d, 'Algorithm': 'ULA', 'Gradient queries': iterations_ula})\n",
    "                \n",
    "                \n",
    "    # Save results\n",
    "    model_dict['df'] = pd.DataFrame(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict['df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "Now that we have all the necessary data we can examine our results to see if they make sense in the context of the paper we are referencing.\n",
    "\n",
    "We begin by generating figures for the individual mixture problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian\n",
    "plt.figure()\n",
    "sns.lineplot(data=models['GaussianMixture']['df'], x='Dimensions', y='Gradient queries', hue='Algorithm').set_title('Gaussian Mixture Model')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaexam",
   "language": "python",
   "name": "adaexam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture model experiment: MCMC and EM\n",
    "In this notebook we perform the experiment to verify that we see an exponential amount of gradient queries for stochastic gradient descent with an increasing parameter $d$, whereas we observe a linear relation in the case of the Metropolis-adjusted Langevin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.plot import plot_points_in_ball, plot_gmm_initializations_2d\n",
    "from src.mixture import GaussianMixture\n",
    "from src.optimizer import em, ula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation and parameter initialization\n",
    "\n",
    "In this section we visualize how data points are generated in the experiment and how the mean parameters are initialized. We will do this for the 2-dimensional setting.\n",
    "\n",
    "We follow the settings explained on page 39, appendix E, in the paper *Sampling can be faster than optimization*. \n",
    "\n",
    "For $d=2$ we have $n=2^d=4$ data points, and a single Gaussian $M=\\lfloor \\log_2(d) \\rfloor = 1$, and thus a single mean parameter. The region that contains the data is the ball (circle) centered at the origin having radius $R=2M=2$.\n",
    "\n",
    "The data points are generated such that the following properties are achieved:\n",
    "\n",
    "1. The clusters need to be adequately separated.\n",
    "2. The number of clusters need to be few ($M=\\lfloor \\log_2 d \\rfloor$ in paper).\n",
    "\n",
    "Observe that the data points have either the x or y coordinate set to 0 since the number of non-zero coefficients for each sampled point is $M=1$.\n",
    "\n",
    "In the following figure we visualize the generated points along with the two different parameter initialization techniques for $d=2$. Initializing the mean of the Gaussian ...\n",
    "\n",
    "1. from the data\n",
    "2. uniformly at random from a ball of radius $R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm_initializations_2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For verification purposes, the following figures show a sample of $1000$ points from the ball of radius $R=2$ in 2- and 3-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points_in_ball(radius=2, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points_in_ball(radius=2, dim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "Now we begin the experiment. The setup is quite simplistic:\n",
    "1. Iterate over parameter $d$.\n",
    "2. Gather our distribution datasets for that parameter $d$.\n",
    "3. Estimate the parameters of the distribution using expectation-maximization.\n",
    "4. Estimate the parameters of the distribution using MALA.\n",
    "5. Save the amount of gradient queries required for both approaches.\n",
    "6. Create line plot of required gradient queries for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 5000\n",
    "MAX_ESTIMATION_ITERATIONS = 1000000\n",
    "EM_ESTIMATE_THRESHOLD = 1e-8\n",
    "EM_ERROR_THRESHOLD = 1e-6\n",
    "ULA_PARAM_ESTIMATE_THRESHOLD = 1e-5\n",
    "ULA_OBJECTIVE_ESTIMATE_THRESHOLD = 1e-8\n",
    "FROM_DATA = True\n",
    "NB_TRIALS = 10\n",
    "NB_TRIALS_ESTIMATE = 20\n",
    "NB_EXPS_ULA = 1\n",
    "MAX_D = 8\n",
    "ERROR_GRADIENT_ESTIMATE = 1e-4\n",
    "GAMMA = None\n",
    "\n",
    "dims = range(2, MAX_D+1)\n",
    "\n",
    "models = {\n",
    "    m.__name__: {\n",
    "        'constructor': m,\n",
    "        'df': None\n",
    "    }\n",
    "    for m in [\n",
    "        GaussianMixture\n",
    "    ]\n",
    "}\n",
    "\n",
    "for name, model_dict in models.items():\n",
    "    # Record results\n",
    "    model_results = []\n",
    "\n",
    "    \n",
    "    # Default iterations to start with\n",
    "    iterations_em = 2\n",
    "    iterations_ula = 2\n",
    "    \n",
    "    # Run for rest of dimensions\n",
    "    for d in tqdm(dims):\n",
    "        # Create finite mixture model problem\n",
    "        model = model_dict['constructor'](d)\n",
    "        \n",
    "        # Run EM\n",
    "        if iterations_em < MAX_ITERATIONS:\n",
    "            \n",
    "            # Estimate underlying set of parameters\n",
    "            em_true_params = None\n",
    "            em_true_params_found = False\n",
    "            em_true_params_iterations = iterations_em*1000\n",
    "            while not em_true_params_found:\n",
    "                \n",
    "                # Assume we have enough iterations until proven otherwise\n",
    "                em_true_params_found = True\n",
    "                \n",
    "                # Run through trials\n",
    "                em_estimate_params = np.zeros((NB_TRIALS_ESTIMATE, em_true_params_iterations, \n",
    "                                               model.params.shape[0], model.params.shape[1]))\n",
    "                em_estimate_objective = np.zeros((NB_TRIALS_ESTIMATE, em_true_params_iterations))\n",
    "                for i in range(NB_TRIALS_ESTIMATE):\n",
    "                    \n",
    "                    # Exit if we failed for given amount of iterations already\n",
    "                    if not em_true_params_found:\n",
    "                        break\n",
    "                    \n",
    "                    # Reset parameters\n",
    "                    model.reset()\n",
    "                    \n",
    "                    # Run EM\n",
    "                    em_param_iterates, em_objective_iterates = em(model, em_true_params_iterations)\n",
    "                    em_estimate_params[i] = em_param_iterates\n",
    "                    em_estimate_objective[i] = em_objective_iterates\n",
    "                    \n",
    "                    # Make sure we don't differ too much from previous estimates\n",
    "                    for j in range(i+1):\n",
    "                        \n",
    "                        # If differ by more than threshold, we reset and increase amount of iterations\n",
    "                        delta = np.linalg.norm(em_estimate_params[j][-1] - em_estimate_params[i][-1], 1)\n",
    "                        if delta > EM_ESTIMATE_THRESHOLD:\n",
    "                            em_true_params_iterations *= 10\n",
    "                            em_true_params_found = False\n",
    "                            print(f'Delta: {delta}')\n",
    "                            print(f'Iterations now: {em_true_params_iterations}')\n",
    "                            break\n",
    "                        \n",
    "                # Just choose any of the 20 iterations\n",
    "                em_true_params = em_estimate_params[-1]\n",
    "                em_true_params_objective = em_estimate_objective[-1]\n",
    "                \n",
    "            # Repeat for the specified amount of trials\n",
    "            for _ in range(NB_TRIALS):\n",
    "            \n",
    "                # Record how many iterations were required\n",
    "                model.reset()\n",
    "                iterations_em = len(em(model, MAX_ITERATIONS, em_true_params_objective[-1], EM_ERROR_THRESHOLD)[0])\n",
    "                model_results.append({'Dimensions': d, 'Algorithm': 'EM', 'Gradient queries': iterations_em})\n",
    "                \n",
    "        # Run ULA (disabled as we couldn't get it to converge in the end)\n",
    "        if False and iterations_ula < MAX_ITERATIONS:\n",
    "            \n",
    "            # Estimate underlying set of parameters\n",
    "            ula_expected_params = None\n",
    "            ula_expected_objective = None\n",
    "            ula_expected_params_found = False\n",
    "            ula_expected_params_iterations = iterations_ula*1000\n",
    "            if ula_expected_params_iterations > MAX_ESTIMATION_ITERATIONS:\n",
    "                raise TimeoutError\n",
    "            while not ula_expected_params_found:\n",
    "                \n",
    "                # Assume we have enough iterations until proven otherwise\n",
    "                ula_expected_params_found = True\n",
    "                \n",
    "                # Run through trials\n",
    "                ula_estimate_params = np.zeros((NB_TRIALS_ESTIMATE,\n",
    "                                               model.params.shape[0], model.params.shape[1]))\n",
    "                ula_estimate_objective = np.zeros(NB_TRIALS_ESTIMATE)\n",
    "                for i in range(NB_TRIALS_ESTIMATE):\n",
    "                    \n",
    "                    # Reset parameters\n",
    "                    model.reset(FROM_DATA)\n",
    "                    \n",
    "                    # Run ULA\n",
    "                    ula_param_iterates, ula_objective_iterates = ula(model, \n",
    "                                                                     ula_expected_params_iterations,\n",
    "                                                                     NB_EXPS_ULA,\n",
    "                                                                     ERROR_GRADIENT_ESTIMATE,\n",
    "                                                                     gamma=GAMMA)\n",
    "                    \n",
    "                    # Compute expectation on parameters\n",
    "                    ula_estimate_params[i] = np.mean(ula_param_iterates)\n",
    "                    ula_estimate_objective[i] = np.mean(ula_objective_iterates)\n",
    "                    \n",
    "                    # Make sure we don't differ too much from previous estimates\n",
    "                    for j in range(i+1):\n",
    "                        \n",
    "                        # If differ by more than threshold, we reset and increase amount of iterations\n",
    "                        delta_params = np.linalg.norm(ula_estimate_params[j] - ula_estimate_params[i], 1)\n",
    "                        delta_objective = abs(ula_estimate_objective[j] - ula_estimate_objective[i])\n",
    "                        if delta_params > ULA_PARAM_ESTIMATE_THRESHOLD or delta_objective > ULA_OBJECTIVE_ESTIMATE_THRESHOLD:\n",
    "                            ula_expected_params_iterations *= 10\n",
    "                            print(f'Delta params: {delta_params}')\n",
    "                            print(f'Delta objective: {delta_objective}')\n",
    "                            print(f'Iterations now: {ula_expected_params_iterations}')\n",
    "                            ula_expected_params_found = False\n",
    "                            break\n",
    "                        \n",
    "                # Just choose any of the 20 iterations\n",
    "                ula_expected_params = ula_estimate_params[-1]\n",
    "                ula_expected_objective = ula_estimate_objective[-1]\n",
    "                \n",
    "            # Repeat for the specified amount of trials\n",
    "            for _ in range(NB_TRIALS):\n",
    "            \n",
    "                # Record how many iterations were required\n",
    "                model.reset(FROM_DATA)\n",
    "                iterations_ula = len(ula(model, MAX_ITERATIONS, NB_EXPS_ULA, ERROR_GRADIENT_ESTIMATE, \n",
    "                                        exp_mu=ula_expected_params, exp_U=ula_expected_objective, \n",
    "                                         timeout=MAX_ITERATIONS, gamma=GAMMA)[0])\n",
    "                model_results.append({'Dimensions': d, 'Algorithm': 'ULA', 'Gradient queries': iterations_ula})\n",
    "                \n",
    "                \n",
    "    # Save results\n",
    "    model_dict['df'] = pd.DataFrame(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict['df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "Now that we have all the necessary data we can examine our results to see if they make sense in the context of the paper we are referencing.\n",
    "\n",
    "We begin by generating figures for the individual mixture problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian\n",
    "plt.figure()\n",
    "sns.lineplot(data=models['GaussianMixture']['df'], x='Dimensions', y='Gradient queries', hue='Algorithm').set_title('Gaussian Mixture Model')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
